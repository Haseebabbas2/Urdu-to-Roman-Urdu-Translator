# ğŸŒ™ Urdu to Roman Urdu Neural Machine Translation

A deep learning-based Neural Machine Translation (NMT) system that translates Urdu script to Roman Urdu (Romanized Urdu). Built with PyTorch and featuring a beautiful Streamlit web interface.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.0+-green.svg)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Training](#training)
- [Evaluation Metrics](#evaluation-metrics)
- [Experiments](#experiments)
- [Author](#author)

## ğŸ” Overview

This project implements a **Sequence-to-Sequence (Seq2Seq)** model with a **Bidirectional LSTM Encoder** and **LSTM Decoder** for translating Urdu text (in Nastaliq script) to Roman Urdu. The model is trained on a curated dataset of Urdu poetry from renowned poets.

### Key Features

- **BiLSTM Encoder**: Captures bidirectional context from Urdu input
- **Character-level Tokenization**: Handles the complexity of Urdu script at the character level
- **Teacher Forcing**: Improves training convergence
- **Multiple Experiments**: Compare different hyperparameter configurations
- **Beautiful Web UI**: Interactive Streamlit interface with glassmorphism design

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Seq2Seq Model                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚    Encoder (BiLSTM)  â”‚     â”‚    Decoder (LSTM)    â”‚         â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  â”‚ â€¢ Embedding Layer    â”‚     â”‚ â€¢ Embedding Layer    â”‚         â”‚
â”‚  â”‚ â€¢ 2-Layer BiLSTM     â”‚â”€â”€â”€â”€â–¶â”‚ â€¢ 4-Layer LSTM       â”‚         â”‚
â”‚  â”‚ â€¢ Dropout (0.3)      â”‚     â”‚ â€¢ Fully Connected    â”‚         â”‚
â”‚  â”‚ â€¢ Hidden Dim: 512    â”‚     â”‚ â€¢ Dropout (0.3)      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â”‚  Input: Urdu Script              Output: Roman Urdu             â”‚
â”‚  "Ø¯Ù„ Ø³Û’ Ø§ØªØ± Ø¬Ø§Ø¦Û’ Ú¯Ø§"       â”€â”€â”€â”€â”€â”€â–¶    "dil se utar jaae ga"    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Component | Specification |
|-----------|---------------|
| Encoder | Bidirectional LSTM |
| Decoder | Unidirectional LSTM |
| Embedding Dimension | 256 |
| Hidden Dimension | 512 |
| Encoder Layers | 2 |
| Decoder Layers | 4 |
| Dropout | 0.3 |

## ğŸ“š Dataset

The dataset consists of Urdu poetry from **30 renowned poets**, including:

- Mirza Ghalib
- Allama Iqbal
- Faiz Ahmad Faiz
- Ahmad Faraz
- Parveen Shakir
- Jaun Eliya
- And many more...

Each poet's folder contains:
- `ur/` - Urdu script version of poems
- `en/` - Roman Urdu transliteration

**Total parallel pairs**: ~3900+ sentence pairs

## ğŸ“ Project Structure

```
NLP/
â”œâ”€â”€ app.py                    # Streamlit web application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py        # Dataset loading & tokenization
â”‚   â”œâ”€â”€ model.py              # Seq2Seq model architecture
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ evaluate.py           # Evaluation metrics (BLEU, CER)
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ exp1_baseline_best.pt       # Baseline model weights
â”‚   â”œâ”€â”€ exp2_small_hidden_best.pt   # Small hidden dim model
â”‚   â””â”€â”€ exp3_high_dropout_best.pt   # High dropout model
â”œâ”€â”€ dataset/                  # Poetry dataset by poet
â”œâ”€â”€ test_data_loader.py       # Data loading tests
â”œâ”€â”€ test_model.py             # Model tests
â”œâ”€â”€ verify_inference.py       # Inference verification script
â””â”€â”€ README.md
```

## âš™ï¸ Installation

### Prerequisites

- Python 3.8+
- pip or conda

### Setup

```bash
# Clone the repository
git clone https://github.com/Haseebabbas2/Urdu-to-Roman-Urdu-Translator.git
cd Urdu-to-Roman-Urdu-Translator

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch streamlit
```

## ğŸš€ Usage

### Web Application

Launch the interactive Streamlit interface:

```bash
streamlit run app.py
```

Then open your browser to `http://localhost:8501` and start translating!

### Programmatic Usage

```python
from src.model import Encoder, Decoder, Seq2Seq
from src.data_loader import Tokenizer, load_data
import torch

# Load tokenizers
pairs = load_data('dataset')
src_tokenizer, tgt_tokenizer = Tokenizer(), Tokenizer()
src_tokenizer.build_vocab([p[0] for p in pairs])
tgt_tokenizer.build_vocab([p[1] for p in pairs])

# Load model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
enc = Encoder(src_tokenizer.vocab_size, 256, 512, 2, 0.3)
dec = Decoder(tgt_tokenizer.vocab_size, 256, 512, 4, 0.3)
model = Seq2Seq(enc, dec, device).to(device)
model.load_state_dict(torch.load('checkpoints/exp1_baseline_best.pt'))
model.eval()
```

### Verify Inference

Run the verification script to test translations:

```bash
python verify_inference.py
```

## ğŸ“ Training

To train the model from scratch:

```bash
python -m src.train
```

This runs three experiments with different configurations:
1. **Baseline**: Standard configuration
2. **Small Hidden**: Reduced hidden dimension (256)
3. **High Dropout**: Increased dropout (0.5)

### Training Configuration

```python
config = {
    'emb_dim': 256,
    'hid_dim': 512,
    'enc_layers': 2,
    'dec_layers': 4,
    'dropout': 0.3,
    'lr': 1e-3,
    'batch_size': 128,
    'epochs': 5,
    'clip': 1
}
```

## ğŸ“Š Evaluation Metrics

The model is evaluated using:

| Metric | Description |
|--------|-------------|
| **BLEU** | Bilingual Evaluation Understudy score (n-gram precision) |
| **CER** | Character Error Rate (Levenshtein distance based) |
| **Perplexity** | Model uncertainty measure (lower is better) |

## ğŸ§ª Experiments

| Experiment | Hidden Dim | Dropout | Description |
|------------|------------|---------|-------------|
| Baseline | 512 | 0.3 | Standard configuration |
| Small Hidden | 256 | 0.3 | Reduced model size |
| High Dropout | 512 | 0.5 | More regularization |

Pre-trained checkpoints for all experiments are available in the `checkpoints/` directory.

## ğŸ› ï¸ Technologies Used

- **PyTorch** - Deep learning framework
- **Streamlit** - Web application framework
- **CUDA/MPS** - GPU acceleration support

## ğŸ‘¤ Author

**Haseeb Abbas**

---

<p align="center">
  <i>Project powered by PyTorch & Streamlit</i>
</p>
